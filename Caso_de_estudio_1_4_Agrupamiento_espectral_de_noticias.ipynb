{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paucaroscanoa/ApiBookAuthor/blob/master/Caso_de_estudio_1_4_Agrupamiento_espectral_de_noticias.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlJnacmgGn-n"
      },
      "source": [
        "# Caso de estudio 1.4: Agrupamiento espectral - Agrupación de noticias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nq3ahQC27ymc"
      },
      "outputs": [],
      "source": [
        "!pip3 install git+https://github.com/mit-nlp/MITIE.git\n",
        "!wget https://github.com/mit-nlp/MITIE/releases/download/v0.4/MITIE-models-v0.2.tar.bz2\n",
        "!tar jxf MITIE-models-v0.2.tar.bz2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_Oso0L_70UF"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import csv\n",
        "\n",
        "#ML\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import cluster\n",
        "\n",
        "#Bibliotecas de web scraping\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "#NLP\n",
        "\n",
        "from mitie import *\n",
        "print('Bibliotecas importadas com sucesso!\\n')\n",
        "print(\"Carregando o modelo NER...\")\n",
        "ner = named_entity_extractor('MITIE-models/english/ner_model.dat')\n",
        "print(\"\\nEtiquetas de saída do modelo NER:\", ner.get_possible_ner_tags())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXMU9GKSXOA_"
      },
      "source": [
        "# Genera la base de datos (Web Scraping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdjoGkujXOBJ"
      },
      "outputs": [],
      "source": [
        "UK_news_url = 'https://www.theguardian.com/uk'\n",
        "#UK_news_url = 'https://web.archive.org/web/20230319001320/http://www.theguardian.com/uk'\n",
        "#UK_news_url = 'http://web.archive.org/web/20230127223105/https://www.theguardian.com/uk'\n",
        "\n",
        "#Download de los links de los distintos temas\n",
        "html_data = requests.get(UK_news_url).text\n",
        "soup = BeautifulSoup(html_data, 'html.parser')\n",
        "\n",
        "# START OF UPDATE IF NEEDED ---------------------\n",
        "url_topics = [f'https://www.theguardian.com{el.find(\"a\")[\"href\"]}' for el in soup.find_all(class_='dcr-4hq641')[1:8]]\n",
        "topics = [el.text.strip('\\n').replace(' ','_') for el in soup.find_all(class_ = 'dcr-4hq641')[1:8]]\n",
        "# END OF UPDATE IF NEEDED ---------------------\n",
        "\n",
        "for i in range(len(topics)):\n",
        "    print('Topic {}: {} ({})'.format(i+1,topics[i],url_topics[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zz0T6E_H5wvU"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McLdTmh1XOBO"
      },
      "outputs": [],
      "source": [
        "def save_to_txt(filename, content):\n",
        "    '''\n",
        "    Creates a new .txt file with as specific name in the Data directory\n",
        "    '''\n",
        "    with open(r\"Data/{}.txt\".format(filename), \"w\") as f:\n",
        "        print(content, file=f)\n",
        "\n",
        "# Cria-se um diretório onde serão salvos os artigos\n",
        "# os.mkdir('C:/Users/Usuario/Desktop/FORMACION/CURSO MIT/CLUSTERING/DATA')\n",
        "os.mkdir('Data/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to request and check if url exists\n",
        "def make_request_with_retry(url, max_retries=3):\n",
        "    for _ in range(max_retries):\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "            return response\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error making request to {url}: {e}\")\n",
        "            time.sleep(5)\n",
        "    return None"
      ],
      "metadata": {
        "id": "CePYMeTRK3lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTootdVXZ5-C"
      },
      "outputs": [],
      "source": [
        "article_titles = []\n",
        "article_contents = []\n",
        "article_topics = []\n",
        "articles_per_topic = 15\n",
        "n = 1\n",
        "\n",
        "for topic, url_topic in zip(topics, url_topics):\n",
        "    # Getting the first 15\n",
        "    soup = BeautifulSoup(requests.get(url_topic).text, 'html.parser')\n",
        "\n",
        "    # START OF UPDATE IF NEEDED ---------------------\n",
        "    url_articles = [f'https://www.theguardian.com{el.find(\"a\")[\"href\"]}' for el in soup.find_all(class_='dcr-c7jt3v')]\n",
        "    # END OF UPDATE IF NEEDED ---------------------\n",
        "\n",
        "    print('\\n{}:'.format(topic))\n",
        "\n",
        "    i = 0\n",
        "    while article_topics.count(topic) < articles_per_topic:\n",
        "        # Check if i is within the range of url_articles\n",
        "        if i >= len(url_articles):\n",
        "            print('Only {} articles found in \"{}\"'.format(article_topics.count(topic), topic))\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            # soup = BeautifulSoup(requests.get(url_articles[i]).text, 'html.parser')\n",
        "            soup = BeautifulSoup(make_request_with_retry(url_articles[i]).text, 'html.parser')\n",
        "\n",
        "            # START OF UPDATE IF NEEDED ---------------------\n",
        "            title = soup.find(class_='dcr-cohhs3').text.strip('\\n')\n",
        "            content = ' '.join([el.text for el in soup.find_all('p')])\n",
        "            # END OF UPDATE IF NEEDED ---------------------\n",
        "\n",
        "            i += 1\n",
        "\n",
        "            if title not in article_titles:\n",
        "                article_titles += [title]\n",
        "                article_contents += [content]\n",
        "                article_topics += [topic]\n",
        "                save_to_txt('title-{}'.format(n), title)\n",
        "                save_to_txt('article-{}'.format(n), content)\n",
        "                save_to_txt('topic-{}'.format(n), topic)\n",
        "                print('{}'.format(title))\n",
        "                n += 1\n",
        "\n",
        "                if round(len(article_titles) / 10) == len(article_titles) / 10:\n",
        "                    print('Article count: {}'.format(len(article_titles)))\n",
        "\n",
        "        except AttributeError:  # Catch AttributeError when title is not found\n",
        "            print('Failed to extract article from {}'.format(url_articles[i]))\n",
        "            i += 1  # Move to the next url_articles\n",
        "\n",
        "df = pd.DataFrame({'topic':article_topics,'title':article_titles,'content':article_contents})\n",
        "df"
      ]
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "df.groupby('topic').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "id": "wnHWbFwxZ8oA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3_qHcBpXOBT"
      },
      "source": [
        "# Importación de la base de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkqzHyhBXOBU"
      },
      "source": [
        "Una vez tenemos la base de datos guardada en carpeta deseada, podemos usar el código del caso de estudio para importar la información."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLKAiyUIXOBV"
      },
      "outputs": [],
      "source": [
        "#número total de artigos a serem processados\n",
        "N = df.shape[0]\n",
        "#para armazenar os temas, títulos e conteúdos das notícias:\n",
        "topics_array = []\n",
        "titles_array = []\n",
        "corpus = []\n",
        "for i in range(1, N+1):\n",
        "    #obtenha o conteúdo do artigo.\n",
        "    with open('Data/article-' + str(i) + '.txt', 'r') as myfile:\n",
        "        d1=myfile.read().replace('\\n', '')\n",
        "        d1 = d1.lower()\n",
        "        corpus.append(d1)\n",
        "    #obtenha o tema original do artigo.\n",
        "    with open('Data/topic-' + str(i) + '.txt', 'r') as myfile:\n",
        "        to1=myfile.read().replace('\\n', '')\n",
        "        to1 = to1.lower()\n",
        "        topics_array.append(to1)\n",
        "    #obtenha o título do artigo.\n",
        "    with open('Data/title-' + str(i) + '.txt', 'r') as myfile:\n",
        "        ti1=myfile.read().replace('\\n', '')\n",
        "        ti1 = ti1.lower()\n",
        "        titles_array.append(ti1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ-OVQfTXOBa"
      },
      "source": [
        "# Generación de atributos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SinUjKUDXOBb"
      },
      "source": [
        "Para generar los atributos de cada instancia (artículo):\n",
        "\n",
        "1. Enlazamos todos los corpus de texto de artículos para determinar todas las palabras únicas que se utilizan en el conjunto de datos.\n",
        "2. Buscamos el subconjunto de las entidades del modelo NER que se encuentra entre las palabras únicas que se utilizan en el conjunto de datos (determinado en el paso 1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQKFw-BKXOBb"
      },
      "outputs": [],
      "source": [
        "#vetor de subconjunto de entidades\n",
        "entity_text_array = []\n",
        "for i in range(1, N+1):\n",
        "    #carregue o arquivo de texto con o conteúdo do artigo e converta-o em uma lista de palavras\n",
        "    tokens = tokenize(load_entire_file(('Data/article-' + str(i) + '.txt')))\n",
        "    #extraia todas as entidades conhecidas do modelo ner mencionado neste artigo\n",
        "    entities = ner.extract_entities(tokens)\n",
        "    #extraia as palavras de entidades reais adicione-as ao vetor\n",
        "    for e in entities:\n",
        "        range_array = e[0]\n",
        "        tag = e[1]\n",
        "        score = e[2]\n",
        "        score_text = \"{:0.3f}\".format(score)\n",
        "        entity_text = \" \".join(tokens[j].decode(\"utf-8\") for j in range_array)\n",
        "        entity_text_array.append(entity_text.lower())\n",
        "#elimine as entidades duplicadas que foram detectadas\n",
        "#entity_text_array = np.unique(entity_text_array)\n",
        "entity_text_array = list(set(entity_text_array))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nHjCRawXOBf"
      },
      "source": [
        "Ahora que ya tenemos la lista de todas las entidades utilizadas en la base de datos, podemos representar cada artículo como un vector que contiene la puntuación de [TF-IDF](https://en.wikipedia.org/wiki/Tf–idf) para cada entidad almacenada en el `entity_text_array`. Esta tarea se puede realizar fácilmente con la librería [scikit-learn](http://scikit-learn.org/stable/) de Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0X8ZtG-hXOBg"
      },
      "outputs": [],
      "source": [
        "vect = TfidfVectorizer(sublinear_tf=True, max_df=0.5, analyzer='word',\n",
        "                       stop_words='english', vocabulary=entity_text_array)\n",
        "corpus_tf_idf = vect.fit_transform(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGlHMSkvXOBk"
      },
      "source": [
        "Ahora que tenemos los artículos representados por sus atributos (puntuaciones de TF-IDF), podemos llevar a cabo el agrupamiento espectral de los mismos usando la librería `scikit-learn` de nuevo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sp0ILK_gXOBl"
      },
      "outputs": [],
      "source": [
        "#Altere n_clusters para o número de grupos desejados\n",
        "n_clusters = 8\n",
        "#Agrupamento espectral\n",
        "spectral = cluster.SpectralClustering(n_clusters= n_clusters,\n",
        "                                      eigen_solver='arpack',\n",
        "                                      affinity=\"nearest_neighbors\",\n",
        "                                      n_neighbors = 10)\n",
        "spectral.fit(corpus_tf_idf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia8cI0j4XOBp"
      },
      "source": [
        "Finalmente las siguientes líneas de código permiten ver el output en el siguiente formato (una línea por artículo):\n",
        "\n",
        "<br>\n",
        "\n",
        "__no. artículo, tema, cluster, título__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLvoJlvsXOBq"
      },
      "outputs": [],
      "source": [
        "if hasattr(spectral, 'labels_'):\n",
        "    cluster_assignments = spectral.labels_.astype(int)\n",
        "    for i in range(0, len(cluster_assignments)):\n",
        "        print(i, topics_array[i], cluster_assignments[i], titles_array[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ws_Zi7TKXOBu"
      },
      "outputs": [],
      "source": [
        "df['predictions'] = cluster_assignments\n",
        "predictions_df = pd.get_dummies(df, columns=['predictions']).drop(['title','content'],axis=1).groupby(['topic']).sum()\n",
        "predictions_df"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "position": {
        "height": "144.545px",
        "left": "938.091px",
        "right": "20px",
        "top": "120px",
        "width": "350px"
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}